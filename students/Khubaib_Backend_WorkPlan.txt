================================================================================
                    YSMAI PROJECT - PERSON A WORK PLAN
                    KHUBAIB - BACKEND & LOGIC OWNER
================================================================================

PROJECT OVERVIEW:
  Real-time HVAC monitoring system with temperature simulation, state management,
  scheduling, and fault injection. Backend exposes stable Python API; UI consumes
  via Streamlit.

================================================================================
DAY 1: CORE SIMULATOR, AGENT, SCHEDULER + TESTS
================================================================================

DELIVERABLES:
  âœ“ TemperatureSimulator class with warmup + drift logic
  âœ“ YSMAI_Agent class with 3-state FSM + debounce with timestamps
  âœ“ Scheduler class with min-heap task queue
  âœ“ Unit tests for each module
  âœ“ Integration tests (simulator â†’ agent â†’ scheduler chain)
  âœ“ API contract specification (JSON schema) shared with Nafis
  âœ“ Sample JSON traces for UI consumption

DETAILED TASKS:

[MORNING - 2-3 hours]

1. TemperatureSimulator Module
   - Class: TemperatureSimulator
   - Constructor params: initial_temp, warmup_duration_sec, drift_rate
   - Methods:
     * tick(elapsed_time_sec) â†’ float (current temp)
     * set_fault_mode(enabled: bool, magnitude: float) â†’ None
     * reset() â†’ None
   - Behavior:
     * Warmup phase: ramp from initial_temp at constant rate for warmup_duration_sec
     * Drift phase: after warmup, add Â±drift_rate per second (simulate aging)
     * Fault mode: inject random spikes/dips when enabled (magnitude param)
   - Output: Temperature value in Â°F (range 50â€“120)
   - File: simulator.py
   - Unit tests:
     * test_warmup_ramp (verify linear rise over duration)
     * test_drift_behavior (verify drift accumulates)
     * test_fault_spike (verify magnitude applied correctly)
     * test_reset (verify state reset to initial)

2. YSMAI_Agent Module
   - Class: YSMAI_Agent
   - Constructor params: threshold_high, threshold_low, debounce_sec
   - States: [NORMAL, ALERT_HIGH, ALERT_LOW]
   - Methods:
     * update(temp: float, timestamp_unix: float) â†’ {state, changed, alert_msg}
     * get_state() â†’ str
     * get_timestamp() â†’ float
   - Behavior:
     * Debounce: state transitions only if condition persists for debounce_sec
     * Track last_transition_time (unix timestamp)
     * Transition logic:
       - NORMAL â†’ ALERT_HIGH if temp > threshold_high for debounce_sec
       - NORMAL â†’ ALERT_LOW if temp < threshold_low for debounce_sec
       - ALERT_* â†’ NORMAL if temp returns to safe zone for debounce_sec
     * Return alert_msg only on state change (not every tick)
   - File: agent.py
   - Unit tests:
     * test_debounce_high_threshold (verify state doesn't jump on single spike)
     * test_debounce_expiry (verify transition after debounce expires)
     * test_state_hysteresis (verify stable state maintained)
     * test_timestamp_tracking (verify timestamps correct)

3. Scheduler Module
   - Class: Scheduler
   - Constructor params: (none initially)
   - Methods:
     * add_task(task_id: str, due_time_unix: float, payload: dict) â†’ None
     * pop_due_tasks(current_time_unix: float) â†’ list of {task_id, payload}
     * clear() â†’ None
   - Implementation: min-heap (use heapq)
   - Behavior:
     * Tasks stored by (due_time, task_id) for ordering
     * pop_due_tasks() returns all tasks with due_time <= current_time
     * Suitable for: persistence checks, data logging, email alerts
   - File: scheduler.py
   - Unit tests:
     * test_task_ordering (verify min-heap returns earliest first)
     * test_pop_due_tasks (verify correct filtering by time)
     * test_empty_queue (verify graceful handling)

[AFTERNOON - 3-4 hours]

4. Integration Test Suite
   - File: test_integration.py
   - Test scenario: Full 10-second simulation
     * Initialize simulator (60Â°F, 5-sec warmup, 0.5Â°F/sec drift)
     * Initialize agent (threshold_high=85, threshold_low=50, debounce=1.5)
     * Initialize scheduler with 3 sample tasks
     * Run tick loop: every 0.5 sec for 10 sec
     * Verify:
       - Simulator ramps up, then drifts
       - Agent transitions states at correct temps
       - Scheduler pops tasks at correct times
       - Alert messages generated only on state change

5. API Contract Specification
   - File: API_CONTRACT.txt
   - Define JSON schema for tick() response:
     {
       "timestamp": float (unix),
       "temperature": float (Â°F),
       "state": str (NORMAL|ALERT_HIGH|ALERT_LOW),
       "state_changed": bool,
       "alert_message": str or null,
       "scheduled_tasks": [
         {
           "task_id": str,
           "payload": object
         }
       ],
       "simulation_time": float (elapsed seconds)
     }
   - Document thresholds:
     * threshold_high: 85Â°F (default)
     * threshold_low: 50Â°F (default)
     * debounce: 1.5 sec (default)
     * drift_rate: 0.5Â°F per minute (default)

6. Sample JSON Traces
   - File: sample_traces.json
   - Generate 3 example traces:
     * Scenario 1: Normal operation (no faults, stable temps)
     * Scenario 2: High-temp alert with recovery
     * Scenario 3: Fault injection â†’ rapid oscillation
   - Each trace: array of 20 tick outputs, human-readable with comments

[END OF DAY 1 - HANDOFF TO NAFIS]

DELIVERABLE CHECKLIST:
  â˜ simulator.py (complete with docstrings)
  â˜ agent.py (complete with docstrings)
  â˜ scheduler.py (complete with docstrings)
  â˜ test_integration.py (all tests passing)
  â˜ API_CONTRACT.txt (frozen schema)
  â˜ sample_traces.json (3 scenarios)
  â˜ Message Nafis with API summary + JSON schema

================================================================================
DAY 2 AM: PYTHON API + SAMPLE DATA PUSH
================================================================================

DELIVERABLES:
  âœ“ Main simulation loop class (SimulationController)
  âœ“ One-step tick() API method
  âœ“ JSON serialization for all outputs
  âœ“ Persistence task scheduling logic
  âœ“ Sample data batch (JSON array) for UI testing
  âœ“ README with API usage examples

DETAILED TASKS:

[MORNING - 3 hours]

1. SimulationController Class
   - File: controller.py (or main simulation file)
   - Constructor params:
     * initial_temp, warmup_duration, drift_rate
     * threshold_high, threshold_low, debounce_sec
     * update_interval_sec (default 0.5)
   - Instance state:
     * simulator, agent, scheduler (from Day 1 modules)
     * current_time (simulation time)
     * fault_injection_enabled, fault_magnitude
   - Methods:
     * tick() â†’ dict (as per API_CONTRACT.txt)
     * set_fault_injection(enabled: bool, magnitude: float) â†’ None
     * reset_simulation() â†’ None
   - Behavior:
     * Each tick() increments simulation time by update_interval_sec
     * Calls simulator.tick(), agent.update(), scheduler.pop_due_tasks()
     * Assembles response dict with all outputs
     * Returns JSON-serializable output

2. Persistence Task Scheduling
   - Add to SimulationController.tick():
     * Auto-schedule "persistence" task every 5 seconds
     * Task payload: {timestamp, temp, state} snapshot
     * scheduler.pop_due_tasks() returns accumulated persistence records
     * Verify in integration test: persistence records appear in tick() output

3. Sample Data Generation
   - File: generate_samples.py (or within test file)
   - Generate 3 JSON arrays:
     * normal_run_100_ticks.json (100 ticks, stable temps)
     * alert_scenario_100_ticks.json (triggers high/low alerts)
     * fault_injection_50_ticks.json (fault mode on, rapid changes)
   - Each entry: full tick() response dict
   - Save to: samples/ folder (create if needed)
   - Include metadata comment: start time, thresholds, description

4. API Usage Examples
   - File: API_EXAMPLES.txt or in README
   - Python example:
     ```python
     from controller import SimulationController
     
     ctrl = SimulationController(
         initial_temp=60,
         warmup_duration=5,
         drift_rate=0.5,
         threshold_high=85,
         threshold_low=50,
         debounce_sec=1.5
     )
     
     for i in range(100):
         result = ctrl.tick()
         print(f"Time: {result['timestamp']}, Temp: {result['temperature']}Â°F, State: {result['state']}")
         if result['state_changed']:
             print(f"  ALERT: {result['alert_message']}")
     ```
   - Explain fault injection:
     ```python
     ctrl.set_fault_injection(enabled=True, magnitude=10.0)
     ```

[END OF DAY 2 AM - HAND OFF TO NAFIS]

DELIVERABLE CHECKLIST:
  â˜ controller.py (SimulationController complete)
  â˜ generate_samples.py or integration test extended
  â˜ samples/normal_run_100_ticks.json
  â˜ samples/alert_scenario_100_ticks.json
  â˜ samples/fault_injection_50_ticks.json
  â˜ API_EXAMPLES.txt with usage code
  â˜ README updated with API intro + links to examples
  â˜ Message Nafis: "API ready, samples in samples/ folder, all JSON validated"

================================================================================
DAY 2 PM: INTEGRATION SUPPORT & MONITORING
================================================================================

STRATEGY: Parallel work with Nafis; available for quick fixes, schema questions.

TASKS (as needed):

1. Monitor Nafis's integration progress
   - Be available for Slack/call if Nafis hits API issues
   - Verify mock fallback data matches schema (no surprise deviations)

2. Prepare for end-of-day sync (15 min)
   - Note any API tweaks requested by UI
   - Prepare quick demo: run 1-minute sim, show tick() output
   - List any remaining blockers

DELIVERABLE CHECKLIST:
  â˜ Responsive to Nafis's integration questions
  â˜ API remains stable (no breaking changes unless agreed)
  â˜ Demo ready for Day 2 PM sync

================================================================================
DAY 3: BUGFIX, TUNING, FINALIZATION
================================================================================

DELIVERABLES:
  âœ“ Bugs from UI integration resolved
  âœ“ Threshold/drift tuning complete
  âœ“ Persistence timing validated
  âœ“ API stability maintained
  âœ“ Final test run (5+ minutes, no crashes)
  âœ“ Code cleanup + final README

DETAILED TASKS:

[MORNING - 2 hours]

1. Bugfix Triage
   - Review blockers list from Day 2 PM sync
   - Prioritize:
     * Crashes/exceptions â†’ fix immediately
     * API contract violations â†’ fix + notify Nafis
     * Performance issues â†’ profile + optimize
     * Minor tweaks â†’ batch and test together
   - Each fix: add unit test to prevent regression

2. Threshold & Drift Tuning
   - Analyze simulator behavior on 5-10 minute runs
   - Adjust drift_rate if too slow/fast (target: noticeable but realistic)
   - Adjust debounce_sec if state flickers or lags (target: smooth transitions)
   - Document final tuned values in README

3. Persistence Timing Validation
   - Run 5-minute simulation
   - Verify persistence tasks scheduled every 5 sec
   - Check no tasks missed, no duplicates
   - Validate persistence records in scheduler output

[AFTERNOON - 2-3 hours]

4. Full End-to-End Test (with Nafis)
   - Launch UI (Streamlit) on laptop A
   - Run backend on laptop B (or same machine, separate terminal)
   - Monitor:
     * API latency (<2 sec per tick) âœ“
     * Chart updates smoothly without gaps âœ“
     * Alerts appear in log with correct timestamps âœ“
     * State transitions reflected immediately in KPI card âœ“
     * Fault injection button triggers rapid changes âœ“
     * Rolling history maintains 300-point cap âœ“
   - Run for 5+ minutes without restart
   - Document any issues found

5. Code Cleanup
   - Remove debug print statements
   - Add docstrings to all public methods
   - Run tests one final time (all passing)
   - Ensure imports are clean (no unused modules)

6. Final README Update
   - Add section: "Architecture Overview"
     * Simulator â†’ Agent â†’ Scheduler â†’ API
     * Data flow diagram (ascii or link to docs)
   - Add section: "Tuned Parameters" (final values)
   - Add section: "Running Tests"
     * Command to run unit tests
     * Command to run integration tests
     * Expected output snippets

[END OF DAY 3]

DELIVERABLE CHECKLIST:
  â˜ All bugs from sync resolved + tested
  â˜ Thresholds & drift tuned + documented
  â˜ 5-minute test run completed (no crashes)
  â˜ Persistence timing validated
  â˜ Final README complete
  â˜ Code cleanup done
  â˜ All unit + integration tests passing
  â˜ Ready for final demo with Nafis

================================================================================
TECHNICAL NOTES
================================================================================

DEPENDENCIES:
  - Python 3.8+
  - heapq (stdlib)
  - json (stdlib)
  - time (stdlib)
  - Optional: pytest (for testing)

FILE STRUCTURE (end of Day 3):
  students/S12345_KhobaitSimran/
    â”œâ”€â”€ main.py                      (SimulationController, entry point)
    â”œâ”€â”€ simulator.py                 (TemperatureSimulator class)
    â”œâ”€â”€ agent.py                     (YSMAI_Agent class)
    â”œâ”€â”€ scheduler.py                 (Scheduler class)
    â”œâ”€â”€ controller.py                (SimulationController class, if separate)
    â”œâ”€â”€ test_integration.py           (integration tests)
    â”œâ”€â”€ test_unit.py                 (unit tests for each module)
    â”œâ”€â”€ API_CONTRACT.txt             (frozen JSON schema)
    â”œâ”€â”€ API_EXAMPLES.txt             (usage examples)
    â”œâ”€â”€ generate_samples.py          (sample data generator)
    â”œâ”€â”€ samples/
    â”‚   â”œâ”€â”€ normal_run_100_ticks.json
    â”‚   â”œâ”€â”€ alert_scenario_100_ticks.json
    â”‚   â””â”€â”€ fault_injection_50_ticks.json
    â”œâ”€â”€ README.md                    (updated with tuning params + arch)
    â””â”€â”€ requirements.txt             (if using virtual env)

ERROR HANDLING:
  - Simulator: no errors (continuous state machine)
  - Agent: log state transitions, handle edge cases (equal to threshold)
  - Scheduler: gracefully handle empty queue, duplicate task_ids
  - API: return error field if any exception occurs (don't crash)

LOGGING:
  - Minimal stdout (only key state transitions + warnings)
  - Use print() for debug, can be toggled with DEBUG flag
  - Sample output:
    [Sim  0.5s] Temp=62.0Â°F, State=NORMAL
    [Sim  3.0s] Temp=75.5Â°F, State=NORMAL, Drift applied
    [Sim  4.0s] Temp=87.2Â°F, Stateâ†’ALERT_HIGH, Alert: "High temperature"

PERSISTENCE:
  - For now: in-memory list of records (task payloads)
  - Future: log to CSV or database
  - Key fields: timestamp, temperature, state

================================================================================
CRITICAL SUCCESS CRITERIA
================================================================================

âœ“ API contract frozen by end of Day 2 AM (no breaking changes after)
âœ“ All simulator/agent/scheduler tests passing by end of Day 1
âœ“ Sample JSON valid, parseable, matches schema
âœ“ 5-minute run without crashes (Day 3)
âœ“ Latency <2 sec per tick (measure in Day 3 test)
âœ“ Alerts trigger only on state change (not every tick)
âœ“ Debounce prevents state flicker (<100ms hysteresis)
âœ“ Persistence tasks auto-scheduled, no manual intervention

================================================================================
COMMUNICATION CHECKLIST
================================================================================

End of Day 1:
  â˜ Send Nafis:
     - API_CONTRACT.txt (JSON schema)
     - sample_traces.json (3 examples)
     - summary: "Core modules complete, API stable, ready for UI mock"

Mid Day 2:
  â˜ Send Nafis:
     - controller.py (SimulationController)
     - samples/ folder (JSON data)
     - API_EXAMPLES.txt (usage code)
     - note: "Stable API, you can integrate now"

End of Day 2:
  â˜ Joint 15-min sync:
     - Demo: run 1-min sim, show JSON output
     - Nafis demos UI with mock data
     - List blockers, assign fixes
     - Agree on any tuning params for Day 3

End of Day 3:
  â˜ Final demo run together
     - Backend + UI live
     - 5-min+ run, all features visible
     - Celebration! ðŸŽ‰

================================================================================
